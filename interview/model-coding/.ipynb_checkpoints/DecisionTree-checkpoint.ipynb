{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce67b8ba",
   "metadata": {},
   "source": [
    "https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/decision_tree.ipynb\n",
    "\n",
    "### 동작\n",
    "- 불순도를 최소화 시키는 특징으로 데이터 셋 분할\n",
    "    - _best_split(X,y) -> best_idx, best_thr\n",
    "    - _gini\n",
    "- 데이터 셋 특정 크기 이하(또는 label 종류 수)되면 정지\n",
    "    - max_depth로 제한\n",
    "- 그 때 평균값으로 회귀/분류 추론?\n",
    "    - Node class에 담기는 정보\n",
    "        - self.predicted_class = predicted_class\n",
    "        - self.feature_index = 0\n",
    "        - self.threshold = 0.0 \n",
    "        - self.left = None\n",
    "        - self.right = None\n",
    "        - is_leaf_node(self)\n",
    "    - leaf 도달할 때까지 노드의 정보로 진행 후, 노드의 레이블 반환\n",
    "\n",
    "### 특징\n",
    "- 장점\n",
    "  - 결정 트리는 해석 및 시각화가 용이\n",
    "  - 수치 데이터와 범주형 데이터를 모두 처리 가능\n",
    "  - 결측값을 처리할 수 있다\n",
    "- 단점 \n",
    "  - 트리가 너무 복잡하거나 데이터에 노이즈나 이상치가 있는 경우 과적합 발생 가능\n",
    "  - 결정 트리는 매우 불균형한 데이터셋이나 많은 관련 없는 특징이 있는 데이터셋에서는 성능이 좋지 않을 수 있음\n",
    "  - 특징과 출력 간의 관계가 매우 비선형적이거나 복잡한 작업에는 적합하지 않을 수 있음\n",
    "- 개선  \n",
    "  - 가지치기(pruning), 앙상블 방법(ensemble methods), 정규화(regularization) 등 일반화 성능을 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5d0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0.0\n",
    "        self.threshold = 0.0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None\n",
    "    \n",
    "        \n",
    "class DecisionTree:\n",
    "    def __init__(self,max_depth=None):\n",
    "        self.max_depth= max_depth\n",
    "    \n",
    "    def _gini(self,y):\n",
    "        n_samples= len(y)\n",
    "        gini= 1- np.sum([ sum(np.where(y==c, 1, 0))/n_samples ** 2 for c in range(len(self.n_classes))])\n",
    "        return gini        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "    def train(self, x,  y):\n",
    "        self.n_classes = np.unique(y)\n",
    "        self.n_features= x.shape[1]\n",
    "        self.root= self._make_tree(x, y)\n",
    "    \n",
    "    def _make_tree(self, x, y, depth):\n",
    "        if depth >= self.max_depth:\n",
    "            return None\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        # 가장 수가 많은 클래스가 해당 노드의 클래스가 된다.\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        \n",
    "        node.feature_index, node.threshold = self._best_split_feature_idx_and_thr(x, y)\n",
    "        left_indices= np.where(x[node.feature_index]<= node.threshold)\n",
    "        right_indices=  np.where(x[node.feature_index] > node.threshold)\n",
    "        node.left= self._make_tree(x[left_indices], y[left_indices], depth+1)\n",
    "        node.left= self._make_tree(x[right_indices], y[right_indices], depth+1)\n",
    "        return node\n",
    "        \n",
    "    def _best_split_feature_idx_and_thr(self, x, y):\n",
    "        best_idx, best_thr, best_gini= None, None, self._gini(y)\n",
    "        total= len(y)\n",
    "        \n",
    "        for idx in range(self.n_features):\n",
    "            # 샘플들의 idx 번째 특성만 추출\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            # DL개, DR개 나눠서 각 집합 불순도합이 얼마나 작아지는 지 본다\n",
    "            # 각 샘플의 idx번째 특성 값 중 제일 불순도 개선이 큰 값이 threshold가 된다.\n",
    "            for thr_idx, thr in enumerate(thresholds):\n",
    "                if i > 0 and thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                dl, dr = x[:thr_idx], x[thr_idx+1:]\n",
    "                ginil, ginir = self._gini(y[:thr_idx]), self._gini(y[thr_idx+1:])\n",
    "                gini= len(dl)/total * ginil + len(dr)/total*ginir\n",
    "                if best_gini > gini:\n",
    "                    best_idx, best_thr, best_gini = idx, thr, gini\n",
    "        return best_idx, best_thr\n",
    "            \n",
    "    def _predict(self, inputs):\n",
    "        node = self.root\n",
    "        # node가 leaf일 때까지 진행 후 리프의 클래스 반환\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20cb0b",
   "metadata": {},
   "source": [
    "# 불순도\n",
    "### 범주형\n",
    "- Information Gain\n",
    "    - 이 값이 클수록 해당 특징이 데이터를 잘 분할한다\n",
    "    - 데이터셋 ( D )에서 특징 ( A )에 대한 정보 이득 \n",
    "    - IG(D, A) = Entropy(D) - ∑v∈Values(A)|Dv||D| · Entropy(Dv)\n",
    "- 지니 불순도(Gini Impurity)\n",
    "    - 값이 0에 가까울수록 노드에 있는 샘플들이 동일한 클래스에 속할 확률이 높다\n",
    "    - Gini(D) = 1 - sum_{i=1}^{C} p_i^2\n",
    "    - ( D )는 데이터셋, ( C )는 클래스의 수, ( p_i )는 클래스 ( i )에 속하는 샘플의 비율\n",
    "- 엔트로피(Entropy)\n",
    "    - 엔트로피 값이 낮을수록 노드가 순수하다\n",
    "    - Entropy(D) = - sum_{i=1}^{C} p_i*log_2(p_i)\n",
    "    - ( D )는 데이터셋, ( C )는 클래스의 수, ( p_i )는 클래스 ( i )에 속하는 샘플의 비율\n",
    "### 회귀\n",
    "- 평균 제곱 오차(MSE) 감소\n",
    "    - 예측 값과 실제 값 간의 차이를 제곱하여 평균한 값\n",
    "    - MSE(D) = sum(y_i-y_pred)^2/N\n",
    "    - ∆MSE = MSE(D) −( |DL|/|D|*MSE(DL) + |DR|/|D|*MSE(DR))\n",
    "- 분산(Variance)\n",
    "    - 회귀 트리에서 분산이 낮을수록 노드가 순수하다\n",
    "    - Variance(D) = sum(y_i - bar{y})^2 / N\n",
    "    - ∆Variance = Variance(D) −( |DL|/|D|*Variance(DL) + |DR|/|D|*Variance(DR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a917991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        impurity = 1 - np.sum([(count / len(y)) ** 2 for count in counts])\n",
    "        return impurity\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        for idx in range(self.n_features_):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        \n",
    "        return best_idx, best_thr\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, *, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0.0 \n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a644e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
