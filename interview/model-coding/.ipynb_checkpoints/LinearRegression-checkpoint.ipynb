{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01544fe",
   "metadata": {},
   "source": [
    "https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/linear_regression.ipynb \n",
    "\n",
    "1. 직접 미분\n",
    "-  \"sum of squared errors\" (SSE) 최소화 -> 미분결과 =0\n",
    "- m = sum((x - x_mean) * (y - y_mean)) / sum((x - x_mean)^2)\n",
    "- b = y_mean - m * x_mean\n",
    "\n",
    "2. 벡터 역행렬\n",
    "- RSS(W)=∥y−XW∥^2 =(y−XW)T(y−XW)\n",
    "- ∂RSS(W)/∂W =0 이 되는 W찾기\n",
    "- RSS(W)=yTy−yTXW−WTXTy+WTXTXW =yTy−2WTXTy+WTXTXW -> W=(XTX)'XTy\n",
    "\n",
    "3. 경사하강법\n",
    "- Add input validation\n",
    "- Add regularization\n",
    "- Use gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd73e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# # y = wx +b\n",
    "# class LinearRegression:\n",
    "#     def __init__(self):\n",
    "#         self.w= np.random.rand(1)\n",
    "#         self.b= np.random.rand(1)       \n",
    "#     def predict(self, x):\n",
    "#         return self.w * x + self.b   \n",
    "#     def train(self, x, y):\n",
    "#         for _x, _y in zip(x, y):\n",
    "#             pred_y= self.predict(_x, _y)\n",
    "#             loss= math.sqrt((_y-pred_y)**2)\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "        x_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for i in range(n):\n",
    "            numerator += (X[i] - x_mean) * (y[i] - y_mean)\n",
    "            denominator += (X[i] - x_mean) ** 2\n",
    "        self.slope = numerator / denominator\n",
    "        self.intercept = y_mean - self.slope * x_mean\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            y_pred.append(self.slope * x + self.intercept)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493e889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearRegression:\n",
    "#     def __init__(self):\n",
    "#         self.W = None\n",
    "#         self.b = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         w = np.linalg.inv(X.T@X) @ X.T @ y\n",
    "#         self.W= w[:, ]\n",
    "#         self.b= w[:, -1]\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X: n x d \n",
    "        '''\n",
    "        # Add bias term to X -> [1 X]\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([np.ones((n, 1)), X])\n",
    "        self.W = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([X, np.ones((n, 1))])\n",
    "        return X @ self.W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d732832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# class LinearRegression:\n",
    "#     def __init__(self, lr=0.01, regl=0.01):\n",
    "#         self.W = None\n",
    "#         self.lr= lr\n",
    "#         self.regl=regl\n",
    "    \n",
    "#     def fit(self, X, Y):\n",
    "#         if len(X) != len(Y):\n",
    "#             raise ValueError(\"sample length doesn't match label length\")\n",
    "            \n",
    "#         for x, y in zip(X, Y)\n",
    "#             y_pred = self.W @ X\n",
    "#             gradients = 2 * (y- y_pred)*(-X) + self.regl * 2 * sum(self.W)\n",
    "#             self.W -= self.lr * gradients\n",
    "            \n",
    "#     def predict(self, X):\n",
    "#         return X @ self.W\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, regul=0):\n",
    "        self.regul = regul\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, num_iter=1000):\n",
    "        # Input validation\n",
    "        if len(X) != len(y) or len(X) == 0:\n",
    "            raise ValueError(\"X and y must have the same length and cannot be empty\")\n",
    "        \n",
    "        # Add bias term to X -> [1 X]\n",
    "        X = np.hstack([np.ones((len(X), 1)), X])\n",
    "\n",
    "        # Initialize W to zeros\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "\n",
    "        # Use gradient descent to minimize cost function\n",
    "        for i in range(num_iter):\n",
    "            # Calculate predicted values\n",
    "            y_pred = np.dot(X, self.W)\n",
    "\n",
    "            # Calculate cost function\n",
    "            cost = np.sum((y_pred - y) ** 2) + self.regul * np.sum(self.W ** 2)\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients = 2 * np.dot(X.T, (y_pred - y)) + 2 * self.regul * self.W\n",
    "\n",
    "            # Update W\n",
    "            self.W = self.W - lr * gradients\n",
    "\n",
    "            if (i % 1000 == 0 ): print(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term to X\n",
    "        X = np.hstack([np.ones((len(X), 1)), X])\n",
    "\n",
    "        # Calculate predicted values\n",
    "        y_pred = np.dot(X, self.W)\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
