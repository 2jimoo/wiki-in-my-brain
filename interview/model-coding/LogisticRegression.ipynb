{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ec5610",
   "metadata": {},
   "source": [
    "https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/logistic_regression.ipynb\n",
    "\n",
    "1. 선형결합 후 sigmoid하면 되는거 아녀..?\n",
    "- 로지스틱 회귀에서 사용하는 비용 함수는 로그 손실(Log Loss) 또는 이진 교차 엔트로피(Binary Cross-Entropy)\n",
    "- cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "- dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "- db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "- MSE아니다~~~ 회귀 아니다~~~\n",
    "\n",
    "2. 경사하강법\n",
    "- 모든 데이터 포인트에 대해 n_iter회 반복\n",
    "- 각 데이터 포인트에 대해 cost = loss + regulation 미분 값에 값 대입하여 gradients 구하기\n",
    "- 가중치에 -lr * gradients 누적\n",
    "\n",
    "3. mini-batch SGD ... optimizer 적용\n",
    "- n_samples: 전체 X 통째로 계산(batch) \n",
    "- batch_size: batch_size 만큼 X 랜덤 샘플링해서 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, regl=0.01):\n",
    "        self.W=None\n",
    "        self.lr=lr\n",
    "        self.regl=regl\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+(np.exp((-z))))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        z= np.dot(self.W, x) \n",
    "        pred_y= sigmoid(z)\n",
    "#         return np.round(y_pred).astype(int)\n",
    "        return pred_y\n",
    "    \n",
    "    def fit(self, X, Y, n_iter=10):\n",
    "        if X is None or len(X) != len(Y):\n",
    "            raise ValueError(\"Sample length is not valid\")\n",
    "        \n",
    "#         initialize weights and bias to zeros  샘플수가 아니라 피쳐수 입니다(^^\n",
    "#         n_samples, n_features = X.shape\n",
    "#         self.weights = np.zeros(n_features)\n",
    "#         self.bias = 0\n",
    "        n =len(X)\n",
    "        self.W=[np.ones(n,1), np.zeros(n,n)]\n",
    "        \n",
    "        for epoch in range(n_iter):\n",
    "            for i in range(n):\n",
    "                x,y=X[i],Y[i]\n",
    "                z= np.dot(self.W, x)\n",
    "                pred_y=sigmoid(z)\n",
    "                # np.sign, np.log, np.sum\n",
    "                grad= 2*(y-pred_y)*(np.dot(x, sigmoid(z)))+2*self.regl*abs(sum(self.W[i]))\n",
    "                self.W -= self.lr * self.W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dedc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize weights and bias to zeros\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # gradient descent optimization\n",
    "        for i in range(self.n_iters):\n",
    "            # calculate predicted probabilities and cost\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            #cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            \n",
    "            # calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # calculate predicted probabilities\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        # convert probabilities to binary predictions\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98784ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.regularization = regularization\n",
    "        self.reg_strength = reg_strength\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        n_batches = n_samples // self.batch_size\n",
    "        for i in range(self.n_iters):\n",
    "            batch_indices = np.random.choice(n_samples, self.batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            z = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "#             cost = (-1 / self.batch_size) * np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))\n",
    "#             if self.regularization == 'l2':\n",
    "#                 reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
    "#                 cost += reg_cost\n",
    "#             elif self.regularization == 'l1':\n",
    "#                 reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(np.abs(self.weights))\n",
    "#                 cost += reg_cost\n",
    "            dw = (1 / self.batch_size) * np.dot(X_batch.T, (y_pred - y_batch))\n",
    "            db = (1 / self.batch_size) * np.sum(y_pred - y_batch)\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.reg_strength / n_samples) * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += (self.reg_strength / n_samples) * np.sign(self.weights)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
