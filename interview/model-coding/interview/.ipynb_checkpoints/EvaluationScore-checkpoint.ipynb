{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e9dea",
   "metadata": {},
   "source": [
    "# 1. MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4820e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, pred_y):\n",
    "    return y-pred_y if y > pred_y else pred_y-y\n",
    "\n",
    "def mse(y, pred_y):\n",
    "    return (y-prey)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b39f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be311e",
   "metadata": {},
   "source": [
    "# 2. CE, BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445a9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y, pred_y):\n",
    "    # -ylogy'-(1-y)log(1-y')\n",
    "    return -y * np.log2(pred_y) - (1-y) *np.log2(1-pred_y)\n",
    "\n",
    "def ce(y, pred_y):\n",
    "    # sum(-ylogy')\n",
    "    return -np.sum(np.dot(y, np.log2(pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6d5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE: 0.23617255147049102\n",
      "CE: 0.46203545934259044\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-10  # Small value to prevent log(0)\n",
    "    bce_loss = np.mean(- y_true * np.log(y_pred + epsilon) - (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return bce_loss\n",
    "\n",
    "# 예제\n",
    "y = np.array([1, 0, 1, 0])\n",
    "pred_y = np.array([0.9, 0.1, 0.8, 0.4])\n",
    "print(\"BCE:\", binary_cross_entropy(y, pred_y))\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-10  # Small value to prevent log(0)\n",
    "    ce_loss = - np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=0))\n",
    "    return ce_loss\n",
    "\n",
    "\n",
    "# 예제\n",
    "y = np.array([1, 0, 0, 1])\n",
    "pred_y = np.array([0.7, 0.2, 0.1, 0.9])\n",
    "print(\"CE:\", cross_entropy(y, pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9e6b6",
   "metadata": {},
   "source": [
    "# 3. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edcd12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정밀도: 모델이 True로 예측한 것 중 실제 True의 비율\n",
    "# 재현율: 실제 True 중 모델이 True로 예측한 비율\n",
    "def f1score(y, pred_y):\n",
    "    # 예측O실제O / 예측X실제X \n",
    "    tp, tn = (y==1 and pred_y ==1), (y==0 and pred_y ==0) \n",
    "    # 예측O실제X / 예측X실제O\n",
    "    fp, fn = (y==0 and pred_y ==1), (y==1 and pred_y ==0)\n",
    "    # 실제 positive 수 / positive 예측 수\n",
    "    precision = (tp+fn) / (tp+fp)\n",
    "    # positive 예측 수 / 실제 positive 수\n",
    "    recall = (tp+fp) / (tp+fn)\n",
    "    \n",
    "    return (precision + recall) / (precision*recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b5a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans\n",
    "import numpy as np\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Calculate confusion matrix\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp + 1e-10)  # Adding epsilon to avoid division by zero\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    \n",
    "    # Calculate F1-score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ab58d",
   "metadata": {},
   "source": [
    "# 4. ROC-AUC, PR-AUC\n",
    "- https://chatgpt.com/c/aeec0b26-ee0c-4e39-9861-b8d495027327 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4071d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y, pred_y):\n",
    "    # 다양한 임계값에서 TPR FPR\n",
    "    pass\n",
    "\n",
    "def pr_auc(y, pred_y):\n",
    "    # 다양한 임계값에서 precison recall \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725c01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.75\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "import numpy as np\n",
    "\n",
    "def roc_auc_score(y_true, y_scores):\n",
    "    # 진짜 양성과 거짓 양성의 모든 점수를 포함하는 리스트\n",
    "    desc_score_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true = np.array(y_true)[desc_score_indices]\n",
    "    y_scores = np.array(y_scores)[desc_score_indices]\n",
    "    \n",
    "    tps = np.cumsum(y_true)\n",
    "    fps = np.cumsum(1 - y_true)\n",
    "    \n",
    "    tpr = tps / tps[-1]\n",
    "    fpr = fps / fps[-1]\n",
    "    \n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "# 예제 데이터\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "print(\"ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b7068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: -0.29166666666666663\n"
     ]
    }
   ],
   "source": [
    "def precision_recall_curve(y_true, y_scores):\n",
    "    desc_score_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true = np.array(y_true)[desc_score_indices]\n",
    "    y_scores = np.array(y_scores)[desc_score_indices]\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.unique(y_scores)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = y_scores >= thresh\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "    return np.array(precisions), np.array(recalls), thresholds\n",
    "\n",
    "def pr_auc_score(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return np.trapz(precision, recall)\n",
    "\n",
    "# 예제 데이터\n",
    "pr_auc = pr_auc_score(y_true, y_scores)\n",
    "print(\"PR-AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc559d",
   "metadata": {},
   "source": [
    "# 5. AP, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979ca2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap(y, pred_y):\n",
    "    pass\n",
    "def mean_ap(y, pred_y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c6a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (AP): 0.25\n",
      "Mean Average Precision (mAP): 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "import numpy as np\n",
    "\n",
    "def precision_recall_curve(y_true, y_scores):\n",
    "    desc_score_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true = np.array(y_true)[desc_score_indices]\n",
    "    y_scores = np.array(y_scores)[desc_score_indices]\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = np.unique(y_scores)\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = y_scores >= thresh\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "    return np.array(precisions), np.array(recalls), thresholds\n",
    "\n",
    "def average_precision_score(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    recall_diff = np.diff(recall, prepend=0)\n",
    "    return np.sum(recall_diff * precision)\n",
    "\n",
    "# 예제 데이터\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "ap = average_precision_score(y_true, y_scores)\n",
    "print(\"Average Precision (AP):\", ap)\n",
    "\n",
    "def mean_average_precision(y_true_list, y_scores_list):\n",
    "    ap_list = []\n",
    "    for y_true, y_scores in zip(y_true_list, y_scores_list):\n",
    "        ap_list.append(average_precision_score(y_true, y_scores))\n",
    "    return np.mean(ap_list)\n",
    "\n",
    "# 여러 클래스에 대한 예제 데이터\n",
    "y_true_list = [\n",
    "    np.array([0, 0, 1, 1]),\n",
    "    np.array([0, 1, 1, 0]),\n",
    "    np.array([1, 0, 0, 1])\n",
    "]\n",
    "y_scores_list = [\n",
    "    np.array([0.1, 0.4, 0.35, 0.8]),\n",
    "    np.array([0.2, 0.6, 0.3, 0.4]),\n",
    "    np.array([0.9, 0.1, 0.2, 0.7])\n",
    "]\n",
    "\n",
    "map_score = mean_average_precision(y_true_list, y_scores_list)\n",
    "print(\"Mean Average Precision (mAP):\", map_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7699d3",
   "metadata": {},
   "source": [
    "# 6. Precision@k, Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a3d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(y, pred_y):\n",
    "    pass\n",
    "def recall_k(y, pred_y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a6fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@k: 0.5\n",
      "Recall@k: 0.5\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "def precision_at_k(y_true, y_scores, k):\n",
    "    top_k = np.argsort(y_scores)[::-1][:k]\n",
    "    relevant = y_true[top_k]\n",
    "    return np.sum(relevant) / k\n",
    "\n",
    "def recall_at_k(y_true, y_scores, k):\n",
    "    top_k = np.argsort(y_scores)[::-1][:k]\n",
    "    relevant = y_true[top_k]\n",
    "    return np.sum(relevant) / np.sum(y_true)\n",
    "\n",
    "precision_k = precision_at_k(y_true, y_scores, 2)\n",
    "recall_k = recall_at_k(y_true, y_scores, 2)\n",
    "\n",
    "print(\"Precision@k:\", precision_k)\n",
    "print(\"Recall@k:\", recall_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eae2e3",
   "metadata": {},
   "source": [
    "# 7. nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1c750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(y, pred_y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b56abe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@k: 0.6131471927654584\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "def dcg_at_k(y_true, y_scores, k):\n",
    "    order = np.argsort(y_scores)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(1, k + 1) + 1)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_at_k(y_true, y_scores, k):\n",
    "    dcg_max = dcg_at_k(y_true, y_true, k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(y_true, y_scores, k) / dcg_max\n",
    "\n",
    "k = 2\n",
    "ndcg = ndcg_at_k(y_true, y_scores, k)\n",
    "print(\"nDCG@k:\", ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ccc6fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@k: 0.6131471927654584\n"
     ]
    }
   ],
   "source": [
    "# interview\n",
    "def ndcg(predictions, labels, k):\n",
    "    # IDCG, 랭킹 0부터 시작하므로 rank+2 사용(?)\n",
    "    ideal_labels= sorted(labels, reverse=True)\n",
    "    ideal_dcg= sum((2**label-1)/np.log2(rank+2) for rank, label in enumerate(ideal_labels[:k]))\n",
    "    \n",
    "    # DCG\n",
    "    pred_indices= sorted(range(len(predictions)),\n",
    "                         key= lambda x: predictions[x],\n",
    "                         reverse=True)\n",
    "    pred_labels= [labels[i] for i in pred_indices]\n",
    "    dcg = sum((2**label-1)/np.log2(rank+2) for rank, label in enumerate(pred_labels[:k]))\n",
    "    \n",
    "    if ideal_dcg:\n",
    "        ndcg = dcg / ideal_dcg\n",
    "    else:\n",
    "        ndcg =0.0\n",
    "    return ndcg\n",
    "\n",
    "k = 2\n",
    "ndcg = ndcg(y_scores, y_true, k)\n",
    "print(\"nDCG@k:\", ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e541c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
