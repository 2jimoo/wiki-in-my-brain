{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d13cf90",
   "metadata": {},
   "source": [
    "# 0. numpy broadcasting\n",
    "- 브로드캐스팅(Broadcasting)은 모양이 다른 배열들 간의 연산이 어떤 조건을 만족했을 때 가능해지도록 배열을 자동적으로 변환하는 것\n",
    "- 차원이 동일하거나 둘 중 하나가 1차원이면 배열 차원을 쭉 늘려서 대강 연산가능하게 해줌\n",
    "- 글고 넘파이는 벡터연산을 지원해서 배열연산보다 빨라용\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/numpy_practice.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607f0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a2ca8",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/linear_regression.ipynb\n",
    "- 정규방정식 사용한다.\n",
    "    - W = (X_T*X)^(−1)*X_T*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "3cb9b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y= wx+b\n",
    "class LinearRegression:\n",
    "    def __init__(self, regul=0.1):\n",
    "        self.regul = regul\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)+self.b\n",
    "    \n",
    "    def fit(self, X, Y, lr=0.01, num_iter=1000):\n",
    "        n_samples, n_features= X.shape\n",
    "        # bias, weight\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        for _ in range(num_iter):\n",
    "            #for x, y in zip(X, Y):\n",
    "            pred_y= np.dot(X, self.W)+self.b\n",
    "            # 회귀: MSE=(y-pred_y)**2\n",
    "            dw= (1 / n_samples) * np.dot(X.T, (y_pred - y))#2*(y-pred_y)*x\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.W -= lr * dw\n",
    "            self.b -= lr * db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b0310e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, regul=0):\n",
    "        self.regul = regul\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, num_iter=1000):\n",
    "        # Input validation\n",
    "        if len(X) != len(y) or len(X) == 0:\n",
    "            raise ValueError(\"X and y must have the same length and cannot be empty\")\n",
    "        \n",
    "        # Add bias term to X -> [1 X]\n",
    "        X = np.hstack([np.ones((len(X), 1)), X])\n",
    "\n",
    "        # Initialize W to zeros\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "\n",
    "        # Use gradient descent to minimize cost function\n",
    "        for i in range(num_iter):\n",
    "            # Calculate predicted values\n",
    "            y_pred = np.dot(X, self.W)\n",
    "\n",
    "            # Calculate cost function\n",
    "            cost = np.sum((y_pred - y) ** 2) + self.regul * np.sum(self.W ** 2)\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients = 2 * np.dot(X.T, (y_pred - y)) + 2 * self.regul * self.W\n",
    "\n",
    "            # Update W\n",
    "            self.W = self.W - lr * gradients\n",
    "\n",
    "            if (i % 1000 == 0 ): print(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term to X\n",
    "        X = np.hstack([np.ones((len(X), 1)), X])\n",
    "\n",
    "        # Calculate predicted values\n",
    "        y_pred = np.dot(X, self.W)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5a2242d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X: n x d \n",
    "        '''\n",
    "        # Add bias term to X -> [1 X]\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([np.ones((n, 1)), X])\n",
    "        self.W = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([X, np.ones((n, 1))])\n",
    "        return X @ self.W\n",
    "\n",
    "def linear_regression(features, labels):\n",
    "    # np.c_[a, b]: (column) 방향으로 배열을 결합 \n",
    "    \n",
    "    # a = np.array([1, 2, 3]) \n",
    "    # b = np.array([4, 5, 6])  \n",
    "    # [[1 4]\n",
    "    # [2 5]\n",
    "    # [3 6]]\n",
    "    \n",
    "    # a = np.array([[1, 2, 3]]) \n",
    "    # b = np.array([[4, 5, 6]])\n",
    "    # [[1 2 3 4 5 6]]\n",
    "    \n",
    "    # a = np.array([1, 2, 3])\n",
    "    # b = np.array([4, 5, 6])\n",
    "    # c = np.array([7, 8, 9])\n",
    "    # [[1 4 7]\n",
    "    # [2 5 8]\n",
    "    # [3 6 9]]\n",
    "    \n",
    "    feat_with_bias= np.c_[\n",
    "        np.ones((features.shapee[0], 1)),\n",
    "        features\n",
    "    ]\n",
    "    # (X_T*X)^(−1)X_Ty\n",
    "    weights= np.linalg.inv(feat_with_bias.T.dot(feat_with_bias)) \\\n",
    "            .dot(feat_with_bias.T) \\\n",
    "            .dot(labels)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c62e0946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n",
      "[inf inf inf inf inf]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3, 4, 5]]).T\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "lr = LinearRegression(regul=0.1)\n",
    "lr.fit(X, y, lr=0.01, num_iter=10000)\n",
    "print(lr.W)  # Output: [ 1.99964292  0.65345474 ]\n",
    "y_pred = lr.predict(X)\n",
    "print(y_pred)  # # Output: [2.65309766, 3.3065524, 3.96000714, 4.61346188, 5.26691662]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6ff99dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0\n",
      "2.879128727013035\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "2.879128727013033\n",
      "[1.99964292 0.65345474]\n",
      "[2.65309766 3.3065524  3.96000714 4.61346188 5.26691662]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegressionGD(regul=0.1)\n",
    "lr.fit(X, y, lr=0.01, num_iter=10000)\n",
    "print(lr.W)  # Output: [ 1.99964292  0.65345474 ]\n",
    "y_pred = lr.predict(X)\n",
    "print(y_pred)  # # Output: [2.65309766, 3.3065524, 3.96000714, 4.61346188, 5.26691662]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa27a4",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/logistic_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c60eadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y= sig(wx+b)\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b= None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z= np.dot(x, self.w)+self.b\n",
    "        pred_y= np.argmax(self.sigmoid(z))\n",
    "        label= np.round(y_pred).astype(int)\n",
    "        return label\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features= X.shape\n",
    "        # bias, weight 별도로 해야한다 왜 어떤건 같이하고 어떤건 따로하지 쩝\n",
    "        #self.w= [np.ones(n_features), np.zeros(n_features)]\n",
    "        self.w=np.zeros(n_features)\n",
    "        self.b= 0\n",
    "        for _ in range(self.n_iters):\n",
    "            #for x, y in zip(X, Y): # 각각 안 할 거면\n",
    "            z= np.dot(X, self.w)+self.b\n",
    "            y_pred= self.sigmoid(z) #np.argmax(self.sigmoid(z))\n",
    "            # 이진분류: BCE = -ylogP + (1-y)log(1-P)\n",
    "            # dloss/dw= (-y) * y'(1-y')/y' * x + (1-y)* (y'(1-y'))/(1-y') * x\n",
    "            # = (-y*(1-y')+(1-y)y') * x = (-y +yy'+y'-yy')*x = (y'-y) * x\n",
    "#             dw= (pred_y- y)*x\n",
    "#             w -= lr* dw\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6cef490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git \n",
    "class LogisticRegressionGD:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize weights and bias to zeros\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # gradient descent optimization\n",
    "        for i in range(self.n_iters):\n",
    "            # calculate predicted probabilities and cost\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            \n",
    "            # calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # calculate predicted probabilities\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        # convert probabilities to binary predictions\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f11ec6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a1e0bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# train model on sample dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = lr.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ab0df73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "lr = LogisticRegressionGD()\n",
    "\n",
    "# train model on sample dataset\n",
    "lr.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = lr.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d1f87",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes\n",
    "https://johnjdailey.medium.com/naive-bayes-classifier-built-in-python-with-numpy-9f05ec26e373 \n",
    "- 어떻게 구현하는지 알아놓기만.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847bb2b",
   "metadata": {},
   "source": [
    "# 4. K-means Clustering\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/k_means.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a23a7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering:\n",
    "    def __init__(self, k=3, max_iter=10):\n",
    "        self.centroids = None\n",
    "        self.k =k\n",
    "        self.max_iter= max_iter\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        cluster_assignments = []\n",
    "        for j in range(len(X)):\n",
    "            distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n",
    "            cluster_assignments.append(np.argmin(distances))\n",
    "        return cluster_assignments\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.centroids= X[np.random.choice(range(len(X)), self.k, replace=False)]\n",
    "        #np.random.choice(X, self.k, replace=False)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            # 각 데이터 샘플에서 centroids까지의 거리 구하여 할당하기\n",
    "            for j in range(len(X)):\n",
    "                dist= np.linalg.norm(self.centroids-X[j], axis=1)\n",
    "                assigned= np.argmin(dist)\n",
    "            #if np.array_equal(prev_assigned, assigned): #np.array_equal는 np배열만 가능함\n",
    "            if i > 0 and np.array_equal(prev_assinged, assigned):\n",
    "                break\n",
    "            # 수렴하지 않았으면 각 클러스터 평균좌표로 센트로이드 업데이트\n",
    "            prev_assinged = np.copy(assigned)\n",
    "            for k in range(self.k):\n",
    "                data= X[np.where(assigned==k)]\n",
    "                if len(data):\n",
    "                    distances= np.linalg.norm(self.centroids[i]-data, axis=1)\n",
    "                    self.centroids[k]= np.mean(distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b04a7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "class KMeans:\n",
    "    def __init__(self, k, max_iterations=100):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Initialize centroids randomly\n",
    "        self.centroids = X[np.random.choice(range(len(X)), self.k, replace=False)]\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # Assign each data point to the nearest centroid\n",
    "            cluster_assignments = []\n",
    "            for j in range(len(X)):\n",
    "                distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n",
    "                cluster_assignments.append(np.argmin(distances))\n",
    "            \n",
    "            # Update centroids\n",
    "            for k in range(self.k):\n",
    "                cluster_data_points = X[np.where(np.array(cluster_assignments) == k)]\n",
    "                if len(cluster_data_points) > 0:\n",
    "                    self.centroids[k] = np.mean(cluster_data_points, axis=0)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if i > 0 and np.array_equal(self.centroids, previous_centroids):\n",
    "                break\n",
    "            \n",
    "            # Update previous centroids\n",
    "            previous_centroids = np.copy(self.centroids)\n",
    "        \n",
    "        # Store the final cluster assignments\n",
    "        self.cluster_assignments = cluster_assignments\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        cluster_assignments = []\n",
    "        for j in range(len(X)):\n",
    "            distances = np.linalg.norm(X[j] - self.centroids, axis=1)\n",
    "            cluster_assignments.append(np.argmin(distances))\n",
    "        \n",
    "        return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7814b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview\n",
    "# inputs:  2D array(n, d) d차원 샘플 n개\n",
    "# centroids: (k, d) d차원 센트로이드 k개\n",
    "# np.linalg.norm(inputs-centroids, axis=2) 차원 달라서 계산 안 됨 브로드캐스팅 되도록 해야함\n",
    "# inputs[:, np.newaxis]: inputs의 차원을 하나 더 늘리는 효과(n,1,d)\n",
    "# inputs[:, np.newaxis]-centroids 결과 (n,k,d)가 되고, np.linalg.norm(-,axis=2) 마지막 축을 따라 유클리드 거리를 계산 (n,k)\n",
    "def kmeans(inputs, k, max_iters=100):\n",
    "    centroids= inputs[np.random.choice(range(len(input)), size=k, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        distances= np.linalg.norm(inputs[:, np.newaxis]-centroids, axis=2)\n",
    "        labels= np.argmin(distances, axis=1)\n",
    "        new_centroids= np.array([inputs[labels==y].mean(axis=0) for i in range(k)])\n",
    "        if np.allclose(centroids, new_centroids):# 두 배열이 허용오차 내에서 요소별로 동일한 경우 True를 반환\n",
    "            break\n",
    "        centroids= new_centroids\n",
    "        \n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "57c5f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[0.         0.        ]\n",
      " [4.8029376  5.45459269]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/615730cj21lfyfd4hqhcdcbr0000gn/T/ipykernel_65856/3594593113.py:30: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  data= X[np.where(assigned==k)]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randn(5,2) + 5\n",
    "x2 = np.random.randn(5,2) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)\n",
    "\n",
    "# Initialize the KMeans object with k=3\n",
    "kmeans = KMeansClustering(k=2)\n",
    "\n",
    "# Fit the k-means model to the dataset\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster assignments for the input dataset\n",
    "cluster_assignments = kmeans.predict(X)\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(cluster_assignments)\n",
    "\n",
    "# Print the learned centroids\n",
    "print(kmeans.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ff96bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[[-4.35239932 -5.6328419 ]\n",
      " [ 4.86049412  4.67505199]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.randn(5,2) + 5\n",
    "x2 = np.random.randn(5,2) - 5\n",
    "X = np.concatenate([x1,x2], axis=0)\n",
    "# Initialize the KMeans object with k=3\n",
    "kmeans = KMeans(k=2)\n",
    "\n",
    "# Fit the k-means model to the dataset\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster assignments for the input dataset\n",
    "cluster_assignments = kmeans.predict(X)\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(cluster_assignments)\n",
    "\n",
    "# Print the learned centroids\n",
    "print(kmeans.centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace85c2",
   "metadata": {},
   "source": [
    "# 5. KNN\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/knn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, X, k=5):\n",
    "        self.k= k\n",
    "        self.X= X\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 최근접 5개 데이터의 레이블 (학습할 게 있나..?)\n",
    "        distances= np.array(np.linalg.norm(self.centroids[i]-data, axis=2))\n",
    "        label= np.mean(np.argsort(distances[:k]))\n",
    "        # 평균X 최빈 레이블O\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d829ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance = distance\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            # Compute distances between the test point and all training points\n",
    "            if self.distance == 'euclidean':\n",
    "                distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "            elif self.distance == 'manhattan':\n",
    "                distances = np.sum(np.abs(self.X_train - x), axis=1)\n",
    "            else:\n",
    "                distances = np.power(np.sum(np.power(np.abs(self.X_train - x), self.distance), axis=1), 1/self.distance)\n",
    "                \n",
    "            # Select the k nearest neighbors\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            \n",
    "            # Assign the class label that appears most frequently among the k nearest neighbors\n",
    "            label = Counter(nearest_labels).most_common(1)[0][0]\n",
    "            y_pred.append(label)\n",
    "        \n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061190ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview\n",
    "def knn(query, candidates, k):\n",
    "    dist=np.linalg.norm(candidates-query, axis=1)\n",
    "    # np.argsort(dist)[:k] # 배열을 정렬하고 그 정렬된 배열의 인덱스를 반환,정확한 순서가 필요할 때\n",
    "    # 배열에서 k번째로 작은 원소를 기준으로 배열을 부분적으로 정렬,순서가 중요하지 않고 상위 k개의 원소만 필요할 때\n",
    "    return np.argpartition(dist, k)[:k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c726f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5 and euclidean distance\n",
    "knn = KNN(k=5, distance='euclidean')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with k=5 and euclidean distance\n",
    "knn = KNN(k=5, distance='euclidean')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Create scatter plots of the test data with colored points representing the true and predicted labels\n",
    "fig, ax = plt.subplots()\n",
    "scatter1 = ax.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='b', cmap='viridis', label=iris.target_names[0])\n",
    "scatter2 = ax.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='g', cmap='viridis', label=iris.target_names[1])\n",
    "scatter3 = ax.scatter(X_test[y_test==2, 0], X_test[y_test==2, 1], c='r', cmap='viridis', label=iris.target_names[2])\n",
    "scatter4 = ax.scatter(X_test[:, 0], X_test[:, 1], c='k', cmap='viridis', marker='x', label='Predicted Label')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('KNN Classifier Results')\n",
    "handles = [scatter1, scatter2, scatter3, scatter4]\n",
    "labels = [h.get_label() for h in handles]\n",
    "ax.legend(handles=handles, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627d5fa",
   "metadata": {},
   "source": [
    "# 6. Decision Tree(★★★★★)\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/decision_tree.ipynb\n",
    "- Gini 계수가 낮을수록 노드의 순도가 높다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "596c37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree_my:\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth=max_depth\n",
    "        self.root= None\n",
    "        \n",
    "    def _gini(self, y): \n",
    "#         classes= np.unique(y)[0]\n",
    "#         classed_portions= np.array([ np.sum(np.where(y==c))/np.sum(y) for c in classes])\n",
    "#         gini= np.sum([cp**2 in cp in classed_portions])\n",
    "#         return gini\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        impurity = 1 - np.sum([(count / len(y)) ** 2 for count in counts])\n",
    "        return impurity\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node= Node_my(predicted_class=predicted_class) \n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        # best_gini, best_feature_idx, best_feature_threshold= self.gini(y), -1, -1\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        # 이 데이터 기준분할 X 전체에 대해\n",
    "        # n_samples, n_features= X.shape\n",
    "        for i in range(self.n_features_):\n",
    "            # data = X[:, i] 스킵하려면 정렬해야함~ \n",
    "            thresholds, classes = zip(*sorted(zip(X[:, i], y)))\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for j in range(1, m):\n",
    "#                 if thresholds[i] == thresholds[i - 1]: thresholds[i]가 0일 때 계산 안하고 넘어가버림..?\n",
    "#                     continue\n",
    "#                 gini_l = self.gini(y[:j]) / (j+1)\n",
    "#                 gini_r = self.gini(y[j+1:]) / (n_samples-j) 가중치(전체 대비 부분셋 크기)여야지 나누는게 아니라..\n",
    "                c = classes[j - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / j) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - j)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (j * gini_left + (m - j) * gini_right) / m\n",
    "                if thresholds[j] == thresholds[j-1]: \n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini, best_idx =gini, i \n",
    "                    best_thr = (thresholds[j] + thresholds[j - 1]) / 2 # thresholds[j]\n",
    "        return best_idx, best_thr\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, *, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0.0 \n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0637f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        impurity = 1 - np.sum([(count / len(y)) ** 2 for count in counts])\n",
    "        return impurity\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        for idx in range(self.n_features_):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        return best_idx, best_thr\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, *, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0.0 \n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "95fe23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview\n",
    "# 정보이득: 특정 피처값을 기반으로 데이터셋을 분할했을 때 얻는 엔트로피의 감소량\n",
    "def entropy(labels):\n",
    "    # np.unique(arr) 고유 요소 추출: 2D 배열에서 고유한 행이나 열을 찾기\n",
    "    # return_index: 고유 요소가 원래 배열에서 나타나는 첫 번째 인덱스를 반환합니다.\n",
    "    # return_inverse: 원래 배열을 재구성하는 데 사용할 수 있는 인덱스를 반환합니다.\n",
    "    # return_counts: 각 고유 요소의 빈도수를 반환합니다.\n",
    "    _, counts= np.unique(labels, return_counts=True)\n",
    "    probs= counts/len(labels)\n",
    "    entropy= -np.sum(probs*np.log2(probs))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(features, labels, split_index):\n",
    "    feat_split= features[:, split_index]\n",
    "    unique_values, counts= np.unique(feat_split, return_counts=True)\n",
    "    probs = counts / len(feat_split)\n",
    "    weight_entropies = [\n",
    "        prob * entropy(labels[feat_split==value]) for value, prob in zip(unique_values, probs)\n",
    "    ]\n",
    "    return entropy(labels) - np.sum(weight_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bdb81dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "456e20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the decision tree\n",
    "tree = DecisionTree_my(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "49d8dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b48e97",
   "metadata": {},
   "source": [
    "# 7. Random Forest\n",
    "https://github.com/sachaMorin/np-random-forest/blob/master/Forest.py \n",
    "- 어떻게 구현하는지 알아놓기만..\n",
    "```\n",
    "max_depth(int): Max depth of trees.\n",
    "no_trees(int): Number of trees.\n",
    "min_samples_split(int): Number of samples in a node to allow\n",
    "split search.\n",
    "min_samples_leaf(int): Number of samples to be deemed a leaf node.\n",
    "feature_search(int): Number of features to search when splitting.\n",
    "bootstrap(boolean): Resample dataset with replacement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6b958",
   "metadata": {},
   "source": [
    "# 8. SVM(★★★★★)\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/svm.ipynb\n",
    "- Hinge Loss(★★★★★)\n",
    "    - max(0,1−y⋅f(x))\n",
    "    - 주로 분류 문제, 특히 Support Vector Machine(SVM)에서 사용\n",
    "- Huber Loss\n",
    "    - 0.5*∣a∣^2 if ∣a∣≤δ else δ(∣a∣-0.5*δ)\n",
    "    - 회귀, 작은 오류에 대해서는 MSE처럼 동작하고, 큰 오류에 대해서는 MAE처럼 동작\n",
    "    - 작은 오류에 대해서는 제곱 오차를, 큰 오류에 대해서는 절대 오차를 사용하여 이상치에 덜 민감하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cb41f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_my:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.w= None\n",
    "        self.b= None\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "    def predict(self, X):\n",
    "        z= np.dot(X, self.w)+self.b\n",
    "        return np.sign(z) # np.where(z < 0 , -1, 1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features= X.shape\n",
    "        #self.w, self.b= np.zeros(n_features), np.ones(n_features)\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # pred_y= np.dot(self.w.T, X)+self.b\n",
    "                # huber_loss = max(0, 1- np.abs(y-pred_y))\n",
    "                # hinge_loss임 huber_loss도 아님ㅜㅜㅋㅋㅋ\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                    # dw= X + regl * 2 * np.sum(w)\n",
    "                    # db= n_samples\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "                    # dw= regl * 2 * np.sum(w)\n",
    "#                 self.w -= lr * dw\n",
    "#                 self.b -= lr * db\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6909a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.lr * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a6cf357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVM_my()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "689da7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVM()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3d5a0",
   "metadata": {},
   "source": [
    "# 9. FFNN\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/feedforward.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58e6e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet_my:\n",
    "    # 모델 파라미터를 받아야한다)\n",
    "    def __init__(self,input_size=2, hidden_size=10, output_size=2):\n",
    "        #self.params={'w1':None, 'w2':None, 'b1':None, 'b2':None}\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "#         exp=np.exp(z)\n",
    "#         exp_sum= np.sum(exp)\n",
    "#         probs= np.array([exp[i]/exp_sum for i in range(len(z))])\n",
    "        # sample별 probs 뽑아내야하기 때문에 probs 1차원 배열로 반환하면 X \n",
    "        # predictions = np.argmax(probs, axis=1)에서 각 샘플에 대해 가장 높은 확률의 인덱스를 찾음\n",
    "        exp_z = np.exp(z)\n",
    "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        a1 = np.where(z1 < 0, 0, z1) # ReLU\n",
    "        z2 = np.dot(a1, self.params['W2']) +self.params['b2']\n",
    "        #a2 = np.where(z2 < 0, 0, z2) # 마지막 층에는 softmax만 취함 활성화X\n",
    "        pred_y= self.softmax(z2)\n",
    "        return pred_y\n",
    "        \n",
    "    \n",
    "    def train(self, X, y, num_epochs=1000, regl=0.03, lr=0.01):\n",
    "#         n_samples, n_features= X.shape\n",
    "#         self.params['w1']=np.zeros(n_features)\n",
    "#         self.params['w2']=np.zeros(n_features)\n",
    "#         self.params['b1']=np.ones(n_features)\n",
    "#         self.params['b2']=np.ones(n_features)        \n",
    "        for _ in range(num_epochs):\n",
    "            # minibatch\n",
    "            z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "            a1 = np.where(z1 < 0, 0, z1) # ReLU: np.maximum(0, z1)\n",
    "            z2 = np.dot(a1, self.params['W2']) +self.params['b2']\n",
    "            probs= self.softmax(z2)\n",
    "            \n",
    "            delta3 = probs\n",
    "            delta3[range(len(X)), y] -= 1\n",
    "            dw2 = np.dot(a1.T, delta3) # np.dot(delta3, a1)\n",
    "            db2 = np.sum(delta3, axis=0) # delta3\n",
    "            \n",
    "            delta2 = np.dot(delta3, self.params['W2'].T)*(a1 > 0)\n",
    "            dw1 =  np.dot(X.T, delta2) # np.dot(delta2, X)\n",
    "            db1 = np.sum(delta2, axis=0) # delta2\n",
    "            \n",
    "            # regularization(regl),optimization(lr)\n",
    "            self.params['W1'] -= lr*dw1\n",
    "            self.params['b1'] -= lr*db1\n",
    "            self.params['W2'] -= lr*dw2\n",
    "            self.params['b2'] -= lr*db2        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32f5e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        z1 = np.dot(X, W1) + b1\n",
    "        a1 = np.maximum(0, z1) # ReLU activation function\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n",
    "        exp_z = np.exp(z2)\n",
    "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        correct_logprobs = -np.log(probs[range(len(X)), y])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        return 1.0/len(X) * data_loss\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate=0.1):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "            a1 = np.maximum(0, z1) # ReLU activation function\n",
    "            z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "            # probs = 1 / (1 + np.exp(-z2)) # Sigmoid activation function\n",
    "            exp_z = np.exp(z2)\n",
    "            probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "            # Backpropagation\n",
    "            delta3 = probs\n",
    "            delta3[range(len(X)), y] -= 1\n",
    "            dW2 = np.dot(a1.T, delta3)\n",
    "            db2 = np.sum(delta3, axis=0)\n",
    "            delta2 = np.dot(delta3, self.params['W2'].T) * (a1 > 0) # derivative of ReLU\n",
    "            dW1 = np.dot(X.T, delta2)\n",
    "            db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "            # Update parameters\n",
    "            self.params['W1'] -= learning_rate * dW1\n",
    "            self.params['b1'] -= learning_rate * db1\n",
    "            self.params['W2'] -= learning_rate * dW2\n",
    "            self.params['b2'] -= learning_rate * db2\n",
    "\n",
    "            # Print loss for monitoring training progress\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.loss(X, y)\n",
    "                print(\"Epoch {}: loss = {}\".format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67e6a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate a toy dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize a neural network\n",
    "net = TwoLayerNet_my(input_size=2, hidden_size=10, output_size=2)\n",
    "\n",
    "# Train the neural network\n",
    "net.train(X, y, num_epochs=1000)\n",
    "\n",
    "# Test the neural network\n",
    "probs = net.forward(X)\n",
    "predictions = np.argmax(probs, axis=1)\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c53ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.4687468424676198\n",
      "Epoch 100: loss = 0.016902136699540674\n",
      "Epoch 200: loss = 0.007560813257671267\n",
      "Epoch 300: loss = 0.004648987758144024\n",
      "Epoch 400: loss = 0.003279740741363126\n",
      "Epoch 500: loss = 0.002501064671430566\n",
      "Epoch 600: loss = 0.002003939050550834\n",
      "Epoch 700: loss = 0.0016631743024980625\n",
      "Epoch 800: loss = 0.001415092124546393\n",
      "Epoch 900: loss = 0.0012278644788697665\n",
      "Predictions:  [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# ans\n",
    "# Generate a toy dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize a neural network\n",
    "net = TwoLayerNet(input_size=2, hidden_size=10, output_size=2)\n",
    "\n",
    "# Train the neural network\n",
    "net.train(X, y, num_epochs=1000)\n",
    "\n",
    "# Test the neural network\n",
    "probs = net.forward(X)\n",
    "predictions = np.argmax(probs, axis=1)\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a2f0b",
   "metadata": {},
   "source": [
    "# 10. Perceptron\n",
    "- https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLC/notebooks/perceptron.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47fa0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_my:\n",
    "    def __init__(self, lr=0.01, n_iter=100):\n",
    "        self.w=None\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def predict(self, X):\n",
    "        z= np.dot(X, self.w[1:])+self.w[0]\n",
    "        y= np.where(z  >= 0.0, 1, -1) #np.where(z < 0, 0, 1)\n",
    "        return y\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        n_samples, n_features= X.shape\n",
    "        self.w= np.zeros(1 + X.shape[1]) # np.array([np.ones(n_features), np.zeros(n_features)])\n",
    "        for _ in range(self.n_iter):\n",
    "            for x, y in zip(X, Y):\n",
    "                #sign까지 통과한 pred 값으로 error 구해야함, abs아님\n",
    "                #z= np.dot(X, self.w[1:])+self.w[0]\n",
    "                #dw= 2*np.abs(y-self.predict(x))\n",
    "                delta= y-self.predict(x)\n",
    "                # weights = delta * x, bias = sum(delta)\n",
    "                self.w[1:] += self.lr*delta*x # -= 아님(정답 방향으로 이동할 것 인데 y-y'했으니까 음수차이나면 음수 그대로 누적)\n",
    "                self.w[0] += self.lr*delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6854be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, lr=0.01, n_iter=100):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros(1 + X.shape[1])\n",
    "        self.errors = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.lr * (target - self.predict(xi))\n",
    "                self.weights[1:] += update * xi\n",
    "                self.weights[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "642987a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2.0, 1.0], [3.0, 4.0], [4.0, 2.0], [3.0, 1.0]])\n",
    "y = np.array([-1, 1, 1, -1])\n",
    "perceptron = Perceptron_my()\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "new_X = np.array([[5.0, 2.0], [1.0, 3.0]])\n",
    "perceptron.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfde78c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ans\n",
    "X = np.array([[2.0, 1.0], [3.0, 4.0], [4.0, 2.0], [3.0, 1.0]])\n",
    "y = np.array([-1, 1, 1, -1])\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "new_X = np.array([[5.0, 2.0], [1.0, 3.0]])\n",
    "perceptron.predict(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc826fe",
   "metadata": {},
   "source": [
    "# 11. 최적화 기법들\n",
    "1. Adam Optimizer\n",
    "2. Regulation\n",
    "3. GD/SGD\n",
    "4. Initialization\n",
    "5. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd70e9",
   "metadata": {},
   "source": [
    "# 12. 평가코드 템플릿\n",
    "- cost, 일정 epoch마다 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f342c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
