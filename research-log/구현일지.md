# 전체 정리
- Term-based scoring
- LSH Regularization
  - Query token embedding과 Document LSH간 비교 속도는 빠르고 성능은 term간 비교와 거의 유사
  - centroid 정의, 최근접 클러스터 구할 때, 샘플링 시 활용
- Incremental Clustering
  - K-means initialization and streaming clustering
  - 초기 최적화하기보다 continual context에서 다른 방법 대비 성능 향상 보여주기

---

# 1차 피드백 정리
- 클러스터 warming up process
  - 초기화 좋은 걸로 시작하게 SS기준 BisectingKMeans 적용
  - LSH 시 SS를 term간 비교로 해서 자동 k detect될 거 같음
- negative sample 정의
  - 클러스터 최소 2개 필요
  - 1개일 때 정의불가: 하지만 예외사항임 고려X
- LSH 적용 히스토리
  - term 기반인데 mean embedding기반 거리 비교
  - 고정길이 프로토타입 생성
    - 클러스터 할당 및 최적 k 결정에 사용

# 샘플링은 최초 임베딩 기준으로 하는데, model update는 새로 임베딩해서 하는게 맞냐 기존 임베딩으로 하는 게 맞냐
- 부차적문제) 기존 임베딩 불러와서 쓰면 트래킹이 됨?
- 샘플링 때 기존 임베딩끼리 샘플링하고, 업데이트할 때 샘플들 새로 임베딩해서 하면 현재 표현 기준으로 업데이트됨


# 대용량 전처리
- HDFS 파일락킹 필요한데 NFS같은 file system은 다중 클라이언트 지원때매 이런거 지원 안함
- 로컬 디렉토리에 작업 후 옮겨야함
  - 시스템에 영향을 주지 않으면서 사용할 수 있는 기본 비휘발성 디렉터리
  - 사용자 홈 디렉터리
  - /usr/local: 사용자 정의 애플리케이션이나 라이브러리 파일을 저장
  - /opt: 서드파티 소프트웨어나 애플리케이션을 설치하기 위한 디렉터리
  - /var: 시스템의 가변 데이터를 저장하는 데 사용됩니다. 로그 파일이나 데이터베이스 파일 등
- 권한 문제
  - ! mkdir -p ~/my_data
  - ~ 기호는 쉘에서 현재 사용자의 홈 디렉터리를 나타내지만, Python 코드에서 사용할 때는 이 기호가 해석되지 않음
    -  os.path.expanduser 함수를 사용하여 홈 디렉터리를 올바르게 해석해야 함 ㅅㅂ


# 클러스터 응집도 지표 
- 내부 거리(internal distance)  
  - 클러스터 내의 모든 샘플 간의 평균 거리로, 낮은 값일수록 응집도가 높음을 나타냅니다.
- Silhouette Score (실루엣 계수)
  - 개별 샘플의 군집 내 응집도와 다른 군집 간의 분리를 평가합니다.
  - 값은 -1에서 1까지이며, 1에 가까울수록 클러스터의 응집도가 높습니다.
- Davies-Bouldin Index (DB 지수)
  - 클러스터 간의 거리와 클러스터 내의 평균 거리의 비율로, 값이 낮을수록 클러스터 간의 분리가 좋고 응집도가 높습니다.
- Dunn Index
  - 클러스터 내의 거리와 클러스터 간의 거리 비율을 나타냅니다.
  - 값이 높을수록 응집도가 높고 분리가 잘 이루어져 있다는 것을 의미합니다.
- Within-Cluster Sum of Squares (WCSS):
  - 각 클러스터의 중심에서 각 샘플까지의 거리 제곱합으로, 낮은 값이 클러스터의 응집도를 높게 나타냅니다.
- Calinski-Harabasz Index
  - 클러스터 내의 분산과 클러스터 간의 분산 비율을 나타내며, 값이 높을수록 응집도가 좋습니다.
 
---

# 배치 사이즈가 성능향상에 기여하는 이유
- 배치당 샘플 다양성 반영, 노이즈 감소(정규화 효과)
- 모델이 작거나 학습이 불안정할 때

# mean pooling
- padding이 있는 경우, padding을 제외한 토큰의 mean pooling을 위해 attention mask를 활용합니다.

# BERT 학습 시에는 어떻게 임베딩? 
- cls? mean pooling? 간단한 mlp(pooler layer?)
- [CLS]분류 작업을 위한 "집계 표현"으로 작용하지만, 이것이 고품질 문장 임베딩 벡터에 대한 최상의 선택은 아니라는 점에 유의해야 합니다 .
- 다른 토큰들의 self-attention에 의한 weighted sum. 모든 입력의 첫 번째 토큰, 대응하는 의미가 없어 집계쵸현이 됨

# long context embedding
- 문서 청킹
  - 똥같은 bert tokenizer는 토크나이징된 리스트로는 가변길이 문자열 처리를 못 함
  - 다시 재결합해서 string으로 넣어주거나, padding+truncate 동시 적용해서 같은 토큰 시퀀스 되게 하거나....
  - is_split_into_words는 이미 단어단위 되어있는가 여부
- overlap으로 문맥정보 넣되, 중복 처리 않도록 함
- [CLS]와 [SEP] 제외
  - last_hidden_state[batch/문장 수, 토큰 수, 768]
 
---
# 시간 병목 지점은 꼭 확인 하자..
- 쓸데없이 계산(클러스터링)
- extend(not list)같이 동작하는데 비정상인 부분..

# 평가 지표
- Success@k: 상위 k개의 결과에 정답이 하나라도 포함되어 있는지 여부를 측정 (정확성 측정).
- Recall@k: 상위 k개의 결과에 정답 문서들이 얼마나 많이 포함되어 있는지 비율을 측정 (포괄성 측정).
