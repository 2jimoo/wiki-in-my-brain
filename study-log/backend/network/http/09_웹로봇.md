# 웹로봇
- 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 프로그램
- 크롤러, 스파이더, 웜, 봇


# 크롤러와 크롤링
- 페이지와 그 페이지가 가리키는 페이지를 재귀적으로 가져오는 박업
- 검색엔진 스파이더
- 루트집합(URL)에서 시작
- 순환피하기(루트와 중복 제거)
  - 트리와 해시테이블
  - URL을 고정크기 해싱하고, 배열 내 presense bit에 매핑
  - 디스크에 체크포인팅 되어있는가
  - 파티셔닝(일부 집합만 할당)
  - 동일 URL 다른 Alias, URL 정규화
  - 파일 시스템 심볼릭 링크 BFS
  - 스로틀링, 일정시간동안 가져올 수 있는 페이지 제한
  - URL최대길이제한(DFS)
  - 블랙리스트
  - 패턴발견, 반복되는 구성요소를 가진 URL은 잠재적 순환으로 보고 정지
  - fingerprint, 페이지 컨텐츠에서 체크섬 계산해서 봤던건지 확인
  - 인간 모니터링

 
# 로봇의 HTTP
- 적절하고 가벼운 요청
- User-Agent, From, Accept, Referer
  - 로봇의 이름, 사용자/관리자 연락처, 어떤 미디어타입에 관심있는지, 현재 URLd포함한 문서의 URL 제공
- Host 헤더
  - 가상호스팅
- 조건부 요청
- 상태코드 대응
- HTML http-equiv tag, 사이트 관리자들과 콘텐츠 저자들이 부여하는 메타지시자


 # 로봇 차단
- robots.txt 선택적 파일 제공, 있으면 우선적으로 처리
- User-Agent로 시작해서 Allow/Disallow URLs
- 주기적으로 캐싱
- 해당 콘텐츠를 작성자가 아니라 웹사이트 관리자가 소유함
  - HTML 문서에 로봇 제어 META 태그 추가
  - NOINDEX 이 페이지를 무시하라, NOFOLLOW 이 페이지에 링크된 페이지 크롤링 하지마라, NOARCHIVE 캐시를 위한 이 페이지 로컬 사본을 만들지 말라
- 로봇 에티켓

# 검색 엔진
- 전 세계 웹페이지 인덱싱 데이터 베이스
  - 단어를 포함하는 문서를 알려줄 수 있음
- 웹 검색 게이트웨잉에 풀 텍스트 색인 질의를 보냄
- 게이트 웨이는 최종사용자를 위한 결과 페이지 생성

# 스푸핑
- 검색 결과에서 내 컨텐츠 높은 순위 만들기 위해 가짜 페이지나 알고리즘 생성
