# 웹로봇
- 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 프로그램
- 크롤러, 스파이더, 웜, 봇


# 크롤러와 크롤링
- 페이지와 그 페이지가 가리키는 페이지를 재귀적으로 가져오는 박업
- 검색엔진 스파이더
- 루트집합(URL)에서 시작
- 순환피하기(루트와 중복 제거)
  - 트리와 해시테이블
  - URL을 고정크기 해싱하고, 배열 내 presense bit에 매핑
  - 디스크에 체크포인팅 되어있는가
  - 파티셔닝(일부 집합만 할당)
  - 동일 URL 다른 Alias, URL 정규화
  - 파일 시스템 심볼릭 링크 BFS
  - 스로틀링, 일정시간동안 가져올 수 있는 페이지 제한
  - URL최대길이제한(DFS)
  - 블랙리스트
  - 패턴발견, 반복되는 구성요소를 가진 URL은 잠재적 순환으로 보고 정지
  - fingerprint, 페이지 컨텐츠에서 체크섬 계산해서 봤던건지 확인
  - 인간 모니터링

 
# 로봇의 HTTP
- 적절하고 가벼운 요청
- User-Agent, From, Accept, Referer
  - 로봇의 이름, 사용자/관리자 연락처, 어떤 미디어타입에 관심있는지, 현재 URLd포함한 문서의 URL 제공
- Host 헤더
  - 가상호스팅
- 조건부 요청
- 상태코드 대응
- HTML http-equiv tag, 사이트 관리자들과 콘텐츠 저자들이 부여하는 메타지시자


 # 로봇 차단
 - robots.txt 선택적 파일 제공
